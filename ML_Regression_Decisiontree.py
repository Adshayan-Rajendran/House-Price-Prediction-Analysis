# -*- coding: utf-8 -*-
"""Copy of Regression_DecisionTree.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ph1Ic15JE5rCyDU11xy888ZyhLjQLKyg

#Machine Learning Assignment
**House Price prediction using Multiple Linear Regression and Decision Trees**
"""

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Import libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib
import plotly.express as px
from sklearn.model_selection import train_test_split, KFold
from sklearn.preprocessing import LabelEncoder
import statsmodels.api as sm
from sklearn.feature_selection import SelectKBest, f_regression, RFECV
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import r2_score
from numpy import mean, absolute, sqrt
import seaborn as sns

# Load the dataset from Google Drive
file_path = "/content/drive/MyDrive/ML/Bengaluru_House_Data.csv"
data = pd.read_csv(file_path)
data

# Check for missing values in each row
rows_with_missing_values = data[data.isnull().any(axis=1)]
print(rows_with_missing_values)

# Count the number of null values in each column
null_values = data.isnull().sum()
print("Number of null values in each column:")
print(null_values)

# Fill missing values with the mean value
balcony_mean = data['balcony'].mean()
bath_mean = data['bath'].mean()
data['balcony'].fillna(round(balcony_mean), inplace=True)
data['bath'].fillna(round(bath_mean), inplace=True)
data['society'] = data['society'].fillna('Unknown')
data.loc[data['availability'] != 'Ready To Move', 'availability'] = 'Not Ready To Move'

# Verify that missing values are filled
print("Number of missing values in 'balcony' column after filling:", data['balcony'].isnull().sum())
print("Number of missing values in 'bath' column after filling:", data['bath'].isnull().sum())
print("Number of missing values in 'society' column after filling:", data['society'].isnull().sum())

# count the no of non numerical values
count_of_non_numerical = pd.to_numeric(data['total_sqft'], errors='coerce').isnull().sum()
print(count_of_non_numerical)

# Remove non-numeric values
data = data[pd.to_numeric(data['total_sqft'], errors='coerce').notnull()]

# check whether still there are null values
null_values = data.isnull().sum()
print("Number of rows with null values:", null_values)

# Remove rows with null values
data = data.dropna()

null_values = data.isnull().sum()
print("Number of rows with null values:", null_values)

# check the how many categories
no_of_categories = data['area_type'].nunique()
print("Number of categories in area_type:", no_of_categories)

no_of_categories = data['availability'].nunique()
print("Number of categories in availability:", no_of_categories)

no_of_categories = data['location'].nunique()
print("Number of categories in location:", no_of_categories)

no_of_categories = data['size'].nunique()
print("Number of categories in size:", no_of_categories)

no_of_categories = data['society'].nunique()
print("Number of categories in society:", no_of_categories)

no_of_categories = data['bath'].nunique()
print("Number of categories in bath:", no_of_categories)

no_of_categories = data['balcony'].nunique()
print("Number of categories in balcony:", no_of_categories)

# Encode categorical variables
data['availability'] = data['availability'].map({'Ready To Move': 1, 'Not Ready To Move': 0})
data['area_type'] = data['area_type'].map({'Plot  Area': 1, 'Carpet Area': 2, 'Built-up  Area': 3, 'Super built-up  Area': 4})

# Check low-frequency categories in 'location' column
category_counts = data['location'].value_counts()
print(category_counts)

# check the no of least frequent locations <= 5
less_category_counts = category_counts[category_counts <= 5]
print(len(less_category_counts))

# retrieve the records <= 5 and selected rows in the 'location' column are assigned the value 'Other location'
low_frequency_locations = category_counts[category_counts <= 5].index
data.loc[data['location'].isin(low_frequency_locations), 'location'] = 'Other location'

# one-hot encoding
encoded_columns = pd.get_dummies(data['location'])

data = pd.concat([data, encoded_columns], axis=1)

data

data = data.drop('location', axis=1)

# one-hot encoding
encoded_columns = pd.get_dummies(data['size'])

data = pd.concat([data, encoded_columns], axis=1)

data = data.drop('size', axis=1)

# Check low-frequency categories in 'society' column
category_counts = data['society'].value_counts()
print(category_counts)

# check the no of least frequent societies <= 10
less_category_counts = category_counts[category_counts <= 10]
print(len(less_category_counts))

# retrieve the categories <= 10 and selected rows in the 'society' column are assigned the value 'Other societies'
categoty_to_rename = category_counts[category_counts <= 10].index
data.loc[data['society'].isin(categoty_to_rename), 'society'] = 'Other societies'

data

# one-hot encoding
encoded_columns = pd.get_dummies(data['society'])

data = pd.concat([data, encoded_columns], axis=1)

data = data.drop('society', axis=1)

data

data.dropna(inplace=True)

null_values = data.isnull().sum()
print("Number of null rows:", null_values)

# convert data types into float to make sure all values are in numerical
data = data.astype(float)

def find_outliers(df):
   q1=df.quantile(0.25)
   q3=df.quantile(0.75)
   IQR=q3-q1
   outliers = df[((df<(q1-1.5*IQR)) | (df>(q3+1.5*IQR)))]
   return outliers

def drop_outliers(df):
   q1=df.quantile(0.25)
   q3=df.quantile(0.75)
   IQR=q3-q1
   not_outliers = df[~((df<(q1-1.5*IQR)) | (df>(q3+1.5*IQR)))]
   outliers_dropped = outliers.dropna().reset_index()
   return outliers_dropped

outliers = find_outliers(data['total_sqft'])

print(f'number_of_outliers: {len(outliers)}')
print(f'max_outlier value: {outliers.max()}')
print(f'min outlier value: {outliers.min()}')

data=data[data.total_sqft.isin(outliers) == False]

outliers = find_outliers(data['bath'])

print(f'number_of_outliers: {len(outliers)}')
print(f'max_outlier value: {outliers.max()}')
print(f'min outlier value: {outliers.min()}')

data=data[data.bath.isin(outliers) == False]

outliers = find_outliers(data['balcony'])

print(f'number_of_outliers: {len(outliers)}')
print(f'max_outlier value: {outliers.max()}')
print(f'min outlier value: {outliers.min()}')

data=data[data.balcony.isin(outliers) == False]

outliers = find_outliers(data['price'])

print(f'number_of_outliers: {len(outliers)}')
print(f'max_outlier value: {outliers.max()}')
print(f'min outlier value: {outliers.min()}')

data=data[data.price.isin(outliers) == False]

data

X = data.drop('price', axis=1)
y = data['price']

# Add a constant column to X for the intercept
X = sm.add_constant(X)

# Fit the OLS (Ordinary Least Squares) model
model = sm.OLS(y, X)
results = model.fit()

# Get the correlation, p-values, and t-values
correlation = results.rsquared
p_values = results.pvalues[1:]  # Exclude the constant term
t_values = results.tvalues[1:]  # Exclude the constant term

# Print the results
print("Correlation:", correlation)
print("P-values:", p_values)
print("T-values:", t_values)

# Set the threshold values for p-values and t-values
p_threshold = 0.05
t_threshold = 1.96

# Select the best features based on p-values and t-values
selected_features = [X.columns[i+1] for i in range(len(p_values)) if p_values[i] < p_threshold and abs(t_values[i]) > t_threshold]
print("Selected Features:", selected_features)

X = X.filter(selected_features)

X

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10)

model_MLL = LinearRegression()

model_MLL.fit(X_train, y_train)

y_predicted = model_MLL.predict(X_test)
# Calculate R-squared value
r_squared = r2_score(y_test, y_predicted)

# Print the R-squared value
print("R-squared value(MLL):", r_squared)

model_DT = DecisionTreeRegressor()
model_DT.fit(X_train, y_train)

y_predicted = model_DT.predict(X_test)

# Calculate R-squared value
r_squared = r2_score(y_test, y_predicted)

# Print the R-squared value
print("R-squared value(DT):", r_squared)